{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20685b94",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807133f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 20:23:03.068550: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-25 20:23:03.072292: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-25 20:23:03.072308: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import string\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "#import scipy.sparse import coo_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d73cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dubai', 'beijing', 'san-francisco', 'new-york-city', 'london', 'las-vegas', 'new-delhi', 'shanghai', '.DS_Store', 'chicago', 'montreal']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"/home/dasharath/Downloads/word2vec_dataset/word2vec_dataset/review_hotels/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e32ca571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25307"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews=[]\n",
    "path=\"/home/dasharath/Downloads/word2vec_dataset/word2vec_dataset/review_hotels/data\"\n",
    "for folders in os.listdir(path):\n",
    "    #print(folders)\n",
    "    if not folders[0].isalnum():\n",
    "        continue\n",
    "    i=0\n",
    "    for filename in os.listdir(path + '/' + folders):\n",
    "        if(i>25):\n",
    "            break\n",
    "        with open(os.path.join(path + '/' + folders,filename),'r', errors='ignore') as f:\n",
    "            for rv in f.readlines():\n",
    "                reviews.append(rv)\n",
    "        i+=1\n",
    "len(reviews)\n",
    "#reviews[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a0544",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d040062c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25307"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_reviews=[]\n",
    "null_reviews=[]\n",
    "for i in range(len(reviews)):\n",
    "    try:\n",
    "        valid_reviews.append(reviews[i][reviews[i].index('\\t')+1:]) #removing date and month from every review\n",
    "    except:\n",
    "         null_reviews.append(reviews[i])\n",
    "len(valid_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "367e3d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Best beach hotel in DUbai\\tWe enjoyed our HONEYMOON at the Habtoor Hotel. Talking about the location: it was the best beach hotel in the area called Dubai Marina. The zone is totaly new and the hotel's beach is the best in the area (we visited some good hotels).The service was 100% high.The room was very nice, with big nice bathroom, with a view to the infinity pool+sea+city. And it was just a standart room. It was clean all the time we stayed there, cleanig the room twice a day, serving fruits in the first day for free and chocolates every day for free. There are a lot of nice restaurants and bars on the hotel's territory.The food at te main restaurant was various. We liked we could choose from lots of fresh juices (orange, watermelon, carrot...) and the food was also fresh.What we didnt like is that the vacation was over and we had to leave :)If we go in Dubai again - we deffinetely choose Habtoor Grand.p.s. There was free bus trips to all the biggest malls in Dubai which really impressed us.\\t\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_reviews[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f938258b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28797eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "lemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49b6d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [lemma.lemmatize(word) for word in tokens if word not in stopwords and word.isalpha()]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54ef90b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/dasharath/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/dasharath/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "25307it [00:14, 1689.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "word_lists=[]\n",
    "from collections import Counter\n",
    "all_words=Counter()\n",
    "for i ,text in tqdm(enumerate(valid_reviews)):\n",
    "    valid_reviews[i]=pre_processing(text)\n",
    "    for word in  valid_reviews[i]:\n",
    "        all_words[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a412c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "uniq_words=[]\n",
    "for w,i in sorted(all_words.items(),key=lambda x:x[1],reverse=True):\n",
    "    if k>10000:\n",
    "        break\n",
    "    uniq_words.append(w)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "884adf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10001it [00:00, 1244318.90it/s]\n"
     ]
    }
   ],
   "source": [
    "unique_word_dict = {}\n",
    "for i, word in tqdm(enumerate(uniq_words)):\n",
    "    unique_word_dict.update({\n",
    "        word: i\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23e378dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hotel 0\n",
      "room 1\n",
      "great 2\n",
      "stay 3\n",
      "good 4\n",
      "would 5\n",
      "staff 6\n",
      "location 7\n",
      "night 8\n",
      "one 9\n",
      "nice 10\n",
      "time 11\n",
      "stayed 12\n",
      "u 13\n",
      "clean 14\n",
      "service 15\n",
      "get 16\n",
      "place 17\n",
      "bed 18\n",
      "day 19\n",
      "breakfast 20\n"
     ]
    }
   ],
   "source": [
    "for w,i in unique_word_dict.items():\n",
    "    if i>20:\n",
    "        break\n",
    "    print(w,i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bacf7bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 25307/25307 [00:41<00:00, 613.68it/s]\n"
     ]
    }
   ],
   "source": [
    "window= 4\n",
    "\n",
    "word_lists = []\n",
    "\n",
    "for j in tqdm(range(len(valid_reviews))):\n",
    "    valid_reviews[j]=[word for word in valid_reviews[j] if word in uniq_words]\n",
    "    for i, word in enumerate(valid_reviews[j]):\n",
    "        for w in range(window):\n",
    "            if i + 1 + w < len(valid_reviews[j]): \n",
    "                word_lists.append([word] + [valid_reviews[j][(i + 1 + w)]])    \n",
    "            if i - w - 1 >= 0:\n",
    "                word_lists.append([word] + [valid_reviews[j][(i - w - 1)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5edc0b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16496744"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de75f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=len(unique_word_dict)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e833f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "    from tensorflow.sparse import SparseTensor as spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0977cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=spa(indices=[[i,unique_word_dict[word_lists[i][0]]] for i in range(128)],values=[1 for i in range(128)],dense_shape=[128,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d9ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d86f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, word_lists, unique_word_dict, batch_size=128,shuffle=False):\n",
    "        'Initialization'\n",
    "        self.word_lists = word_lists\n",
    "        self.batch_size = batch_size\n",
    "        self.unique_word_dict = unique_word_dict\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.word_lists) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.word_lists[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.word_lists))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, word_lists_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        n=len(self.unique_word_dict)\n",
    "        n1=len(word_lists_temp)\n",
    "        \n",
    "        X = spa(indices=[[i,self.unique_word_dict[word_lists_temp[i][0]]] for i in range(n1)],values=[1 for i in range(n1)],dense_shape=[n1,n])\n",
    "        Y = spa(indices=[[i,self.unique_word_dict[word_lists_temp[i][1]]] for i in range(n1)],values=[1 for i in range(n1)],dense_shape=[n1,n])\n",
    "        return X,tf.sparse.to_dense(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61e0a6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 20:36:28.600894: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-25 20:36:28.600917: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-25 20:36:28.600934: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (linux): /proc/driver/nvidia/version does not exist\n",
      "2022-07-25 20:36:28.601099: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/tmp/ipykernel_17247/3527372978.py:14: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n",
      "/home/dasharath/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 50), dtype=float32), dense_shape=Tensor(\"gradient_tape/model/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128880/128880 [==============================] - 2511s 19ms/step - loss: 7.1658\n",
      "Epoch 2/4\n",
      "128880/128880 [==============================] - 2511s 19ms/step - loss: 7.1099\n",
      "Epoch 3/4\n",
      "128880/128880 [==============================] - 2549s 20ms/step - loss: 7.1132\n",
      "Epoch 4/4\n",
      "128880/128880 [==============================] - 2521s 20ms/step - loss: 7.1112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ab8fd31c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "inp_out_nodes=len(unique_word_dict)\n",
    "embed_size = 50\n",
    "\n",
    "data_generator=DataGenerator(word_lists,unique_word_dict,batch_size=128)\n",
    "\n",
    "inp = Input(shape=(inp_out_nodes,),sparse=True)\n",
    "x = Dense(units=embed_size, activation='linear')(inp)\n",
    "x = Dense(units=inp_out_nodes, activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=data_generator,\n",
    "    use_multiprocessing=True,\n",
    "    workers=6,\n",
    "    epochs=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fb291e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 50)\n"
     ]
    }
   ],
   "source": [
    "weights = model.get_weights()[0]\n",
    "print(weights.shape)\n",
    "# Creating a dictionary to store the embeddings in. The key is a unique word and \n",
    "# the value is the numeric vector\n",
    "embedding_dict = {}\n",
    "for word in uniq_words: \n",
    "    embedding_dict.update({\n",
    "        word: weights[unique_word_dict.get(word)]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6680ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "def cossim(x,y):\n",
    "    return np.dot(x,y)/norm(x)*norm(y)\n",
    "def top(x):\n",
    "    tops=[]\n",
    "    for w,i in embedding_dict.items():\n",
    "        tops.append([cossim(i,x),w])\n",
    "    tops.sort(reverse=True)\n",
    "    return tops[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2c3789f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.9499655, 'polite'], [10.1619625, 'courteous'], [9.837849, 'friendly'], [9.711471, 'professional'], [9.218165, 'knowledgeable'], [9.184785, 'welcoming'], [9.153411, 'accomodating'], [9.053712, 'staff'], [9.040276, 'efficient'], [8.98842, 'helpful']]\n"
     ]
    }
   ],
   "source": [
    "print(top(embedding_dict['polite']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b404a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.337451, 'dirty'], [8.56621, 'filthy'], [8.040773, 'stained'], [7.9002037, 'stain'], [7.527453, 'carpet'], [7.330088, 'gross'], [7.1776714, 'vacuumed'], [7.084972, 'dust'], [7.0503836, 'moldy'], [7.0153537, 'nasty']]\n"
     ]
    }
   ],
   "source": [
    "print(top(embedding_dict['dirty']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4816e4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.352672, 'france'], [3.0372612, 'draycott'], [2.9666753, 'singapore'], [2.9248126, 'england'], [2.8994017, 'leg'], [2.8889282, 'canada'], [2.812708, 'newbie'], [2.8088207, 'dream'], [2.803924, 'caribbean'], [2.7674694, 'paragon']]\n"
     ]
    }
   ],
   "source": [
    "print(top(embedding_dict['france']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57934e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.495183, 'shock'], [3.137311, 'screwed'], [3.1298199, 'frazzled'], [3.1182957, 'glance'], [3.080535, 'mistake'], [3.007596, 'visa'], [2.9525337, 'cc'], [2.926972, 'impression'], [2.902279, 'realised'], [2.8975484, 'delayed']]\n"
     ]
    }
   ],
   "source": [
    "print(top(embedding_dict['shock']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc66226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
